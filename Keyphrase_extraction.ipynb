{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kleis - Keyphrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module to load corpus and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kleis.resources.dataset as kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus\n",
    "\n",
    "To load the corpus use the following method.\n",
    "\n",
    "```\n",
    "kl.load_corpus()\n",
    "```\n",
    "\n",
    "The corpus loaded by default is [SemEval 2017 Task 10](https://scienceie.github.io/resources.html), at the moment it is the only one available. Files for the corpus are shearched in ~/kleis_data/corpus/semeval2017-task10 or ./kleis_data/corpus/semeval2017-task10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kleis.resources.semeval2017.SemEval2017 object at 0x7f1af08aa0b8>\n"
     ]
    }
   ],
   "source": [
    "default_corpus = kl.load_corpus()\n",
    "print(default_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info about dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: semeval2017-task10\n",
      "Config: {'train-labeled': '/home/snov/Projects/univ-paris13/nlp/code/kleis-keyphrase-extraction/src/kleis/kleis_data/corpus/semeval2017-task10/train2/', 'dev-labeled': '/home/snov/Projects/univ-paris13/nlp/code/kleis-keyphrase-extraction/src/kleis/kleis_data/corpus/semeval2017-task10/dev/', 'test-unlabeled': '/home/snov/Projects/univ-paris13/nlp/code/kleis-keyphrase-extraction/src/kleis/kleis_data/corpus/semeval2017-task10/scienceie2017_test_unlabelled/', 'test-labeled': '/home/snov/Projects/univ-paris13/nlp/code/kleis-keyphrase-extraction/src/kleis/kleis_data/corpus/semeval2017-task10/semeval_articles_test/'}\n",
      "Name: semeval2017-task10\n",
      "Config: {'train-labeled': '/home/snov/Projects/univ-paris13/nlp/code/kleis-keyphrase-extraction/src/kleis/kleis_data/corpus/semeval2017-task10/train2/', 'dev-labeled': '/home/snov/Projects/univ-paris13/nlp/code/kleis-keyphrase-extraction/src/kleis/kleis_data/corpus/semeval2017-task10/dev/', 'test-unlabeled': '/home/snov/Projects/univ-paris13/nlp/code/kleis-keyphrase-extraction/src/kleis/kleis_data/corpus/semeval2017-task10/scienceie2017_test_unlabelled/', 'test-labeled': '/home/snov/Projects/univ-paris13/nlp/code/kleis-keyphrase-extraction/src/kleis/kleis_data/corpus/semeval2017-task10/semeval_articles_test/'}\n",
      "len(Train): None\n",
      "len(Dev): None\n",
      "len(Test): None\n"
     ]
    }
   ],
   "source": [
    "print(\"Name:\", default_corpus.name)\n",
    "print(\"Config:\", default_corpus.config)\n",
    "\n",
    "print(\"Name:\", default_corpus.name)\n",
    "print(\"Config:\", default_corpus.config)\n",
    "print(\"len(Train):\", len(default_corpus.train) if default_corpus.train else None)\n",
    "# print(\"Train:\", default_corpus.train.popitem())\n",
    "print(\"len(Dev):\", len(default_corpus.dev) if default_corpus.dev else None)\n",
    "# print(\"Dev:\", default_corpus.dev.popitem())\n",
    "print(\"len(Test):\", len(default_corpus.test) if default_corpus.test else None)\n",
    "# print(\"Test:\", default_corpus.test.popitem())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoS tag sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of PoS sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_corpus.load_pos_sequences()\n",
    "pos_sequences = default_corpus.pos_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('JJ NN', {'tags': ['JJ', 'NN'], 'count': 526})\n",
      "('NN NN', {'tags': ['NN', 'NN'], 'count': 444})\n",
      "('NN IN NN NN', {'tags': ['NN', 'IN', 'NN', 'NN'], 'count': 9})\n",
      "('NN', {'tags': ['NN'], 'count': 684})\n",
      "('JJ NN NN', {'tags': ['JJ', 'NN', 'NN'], 'count': 200})\n",
      "('NN NN NN', {'tags': ['NN', 'NN', 'NN'], 'count': 89})\n",
      "('NNP NN', {'tags': ['NNP', 'NN'], 'count': 177})\n",
      "('NNP', {'tags': ['NNP'], 'count': 627})\n",
      "('JJ NNS NNS', {'tags': ['JJ', 'NNS', 'NNS'], 'count': 3})\n",
      "('JJ NN NNS', {'tags': ['JJ', 'NN', 'NNS'], 'count': 130})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([str(pos_seq) for pos_seq in pos_sequences.items()][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrase extraction with Kleis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document example...\n",
      "\n",
      "Content to label:\n",
      "\n",
      " Information extraction is the process of extracting structured data from unstructured text, which is relevant for several end-to-end tasks, including question answering. This paper addresses the tasks of named entity recognition (NER), a subtask of information extraction, using conditional random fields (CRF). Our method is evaluated on the ConLL-2003 NER corpus.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Information extraction is the process of extracting structured data from unstructured text, \\\n",
    "which is relevant for several end-to-end tasks, including question answering. \\\n",
    "This paper addresses the tasks of named entity recognition (NER), \\\n",
    "a subtask of information extraction, using conditional random fields (CRF). \\\n",
    "Our method is evaluated on the ConLL-2003 NER corpus.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Document example...\\n\")\n",
    "print(\"Content to label:\\n\\n\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train or load model\n",
    "\n",
    "Before labeling keyphrases use the following method. \n",
    "\n",
    "```\n",
    "default_corpus.training()\n",
    "```\n",
    "\n",
    "It loads the model to label keyphrases or start the training, \n",
    "\n",
    "It is recomended to use the default arguments. Note that if the model with other arguments doesn't exists, is going to be generated and this process could take several time.  \n",
    "\n",
    "```\n",
    "Default: filter_min_count = 3\n",
    "Default: tagging_notation=\"BILOU\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train or load model\n",
    "default_corpus.training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling text\n",
    "\n",
    "To label text with the trained model use the following method.\n",
    "\n",
    "```\n",
    "default_corpus.label_text(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling\n",
    "keyphrases = default_corpus.label_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyphrases are returned as a list describing the keyphrase. \n",
    "\n",
    "Each element is a tuple with fields.\n",
    "```\n",
    "(\"Keyphrase ID\", (\"Label\", Start, End), \"Text of keyphrase\")\n",
    "    string         string   int   int        string \n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "[\n",
    "    ('T4', ('KEYPHRASE', (293, 309)), 'holographic mask'), \n",
    "    ('T10', ('KEYPHRASE', (735, 751)), 'holographic mask')\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of labeled keyphrases:\n",
      "\n",
      " [('T3', ('KEYPHRASE', (0, 22)), 'Information extraction'), ('T4', ('KEYPHRASE', (150, 168)), 'question answering'), ('T5', ('KEYPHRASE', (210, 228)), 'entity recognition'), ('T6', ('KEYPHRASE', (249, 271)), 'information extraction'), ('T24', ('KEYPHRASE', (230, 233)), 'NER'), ('T25', ('KEYPHRASE', (306, 309)), 'CRF'), ('T28', ('KEYPHRASE', (279, 304)), 'conditional random fields')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of labeled keyphrases:\\n\\n\", keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result could be formated as [brat](http://brat.nlplab.org/examples.html#annotation-examples) with the method.\n",
    "\n",
    "```\n",
    "kl.keyphrases2brat(keyphrases)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T3\tKEYPHRASE 0 22\tInformation extraction\n",
      "T4\tKEYPHRASE 150 168\tquestion answering\n",
      "T5\tKEYPHRASE 210 228\tentity recognition\n",
      "T6\tKEYPHRASE 249 271\tinformation extraction\n",
      "T24\tKEYPHRASE 230 233\tNER\n",
      "T25\tKEYPHRASE 306 309\tCRF\n",
      "T28\tKEYPHRASE 279 304\tconditional random fields\n"
     ]
    }
   ],
   "source": [
    "print(kl.keyphrases2brat(keyphrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with the SemEval 2017 Task 10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible values to select PoS sequences are the counts of occurrences of each PoS sequence in the train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of each PoS Sequence in the train dataset\n",
      " [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 21, 22, 24, 25, 26, 27, 28, 34, 37, 39, 50, 63, 89, 103, 107, 130, 177, 200, 219, 283, 335, 444, 526, 627, 684]\n"
     ]
    }
   ],
   "source": [
    "filter_limits = sorted(set([pos_seq[\"count\"] for pos_seq in pos_sequences.values()]))\n",
    "print(\"Occurrences of each PoS Sequence in the train dataset\\n\", filter_limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples iterates over the dev and tests dataset for SemEval 2017 Task 10, the outputs are saved under output-dev/ and output-test/ in the main directory of this package. The results an be evaluated using the script ./eval_example_output_semeval2017.sh\n",
    "\n",
    "Be sure to have the corpus in the correct path and load each datasets.\n",
    "\n",
    "```\n",
    "default_corpus.load_train()\n",
    "default_corpus.load_test()\n",
    "default_corpus.load_dev()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_corpus.load_train()\n",
    "default_corpus.load_test()\n",
    "default_corpus.load_dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 3\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 4\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 5\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 6\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 7\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 8\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 9\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 10\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 11\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 12\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 13\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 14\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 15\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 16\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 17\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 20\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 21\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 22\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 24\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 25\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 26\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 27\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 28\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 34\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 37\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 39\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 50\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 63\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 89\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 103\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 107\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 130\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 177\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 200\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 219\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 283\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 335\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 444\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 526\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 627\n",
      "Written docs: 50           \n",
      " - Minimum occurence of PoS sequences to train the model: 684\n"
     ]
    }
   ],
   "source": [
    "for fl in filter_limits:\n",
    "    default_corpus.training(filter_min_count=fl)\n",
    "    output = \"output-dev/%s/\" % fl\n",
    "    if not kl.path_exists(output):\n",
    "        os.makedirs(output)\n",
    "    for i, (key, tmp_dataset) in enumerate(default_corpus.dev.items()):\n",
    "        text = tmp_dataset[\"raw\"][\"txt\"]\n",
    "        keyphrases = default_corpus.label_text(text)\n",
    "        with open(output + key + \".ann\", \"w\", encoding=\"utf-8\") as fout:\n",
    "            fout.write(kl.keyphrases2brat(keyphrases))\n",
    "        with open(output + key + \".txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "            fout.write(kl.keyphrases2brat(keyphrases))\n",
    "    print(\"Written docs: %d \\\n",
    "          \\n - Minimum occurence of PoS sequences to train the model: %d\" % (i + 1, fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 3\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 4\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 5\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 6\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 7\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 8\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 9\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 10\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 11\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 12\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 13\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 14\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 15\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 16\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 17\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 20\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 21\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 22\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 24\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 25\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 26\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 27\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 28\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 34\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 37\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 39\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 50\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 63\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 89\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 103\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 107\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 130\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 177\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 200\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 219\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 283\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 335\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 444\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 526\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 627\n",
      "Written docs: 100           \n",
      " - Minimum occurence of PoS sequences to train the model: 684\n"
     ]
    }
   ],
   "source": [
    "for fl in filter_limits:\n",
    "    default_corpus.training(filter_min_count=fl)\n",
    "    output = \"output-test/%s/\" % fl\n",
    "    if not kl.path_exists(output):\n",
    "        os.makedirs(output)\n",
    "    for i, (key, tmp_dataset) in enumerate(default_corpus.test.items()):\n",
    "        text = tmp_dataset[\"raw\"][\"txt\"]\n",
    "        keyphrases = default_corpus.label_text(text)\n",
    "        with open(output + key + \".ann\", \"w\", encoding=\"utf-8\") as fout:\n",
    "            fout.write(kl.keyphrases2brat(keyphrases))\n",
    "        with open(output + key + \".txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "            fout.write(kl.keyphrases2brat(keyphrases))\n",
    "    print(\"Written docs: %d \\\n",
    "          \\n - Minimum occurence of PoS sequences to train the model: %d\" % (i + 1, fl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
